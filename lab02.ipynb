{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Define the strategy for distributed training\nstrategy = tf.distribute.MirroredStrategy()\n\n# Load the Patch Camelyon dataset\ndataset, info = tfds.load('patch_camelyon', split='train', as_supervised=True, with_info=True)\n\n# Split the dataset into training and validation sets\ntrain_dataset = dataset.take(10000)\nval_dataset = dataset.skip(10000)\n\n# Define batch size per replica\nglobal_batch_size = 32\nbatch_size = global_batch_size // strategy.num_replicas_in_sync\n\n# Prepare datasets for distributed training\ntrain_dataset = train_dataset.cache().shuffle(buffer_size=10000).batch(batch_size)\nval_dataset = val_dataset.cache().batch(batch_size)\n\n# Create and compile the model inside the strategy scope\nwith strategy.scope():\n    model = tf.keras.applications.ResNet50(weights=None, input_shape=(96, 96, 3), classes=2)\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n# Define number of epochs\nnum_epochs = 10\n\n# Train the model\nmodel.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(val_dataset)\nprint(\"Validation Loss:\", loss)\nprint(\"Validation Accuracy:\", accuracy)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-04T12:46:40.318175Z","iopub.execute_input":"2024-02-04T12:46:40.318500Z","iopub.status.idle":"2024-02-04T13:39:21.813041Z","shell.execute_reply.started":"2024-02-04T12:46:40.318472Z","shell.execute_reply":"2024-02-04T13:39:21.812110Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-04 12:46:42.021970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-04 12:46:42.022151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-04 12:46:42.147172: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 7.48 GiB (download: 7.48 GiB, generated: Unknown size, total: 7.48 GiB) to /root/tensorflow_datasets/patch_camelyon/2.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50a3f8b3df7d4635b168ff4ad9e640eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8e16b8cd604888b5808187fd10775f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b5b74e9c9e485cbc5b64cd2acf6cb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/32768 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/patch_camelyon/2.0.0.incompleteDEKBZI/patch_camelyon-test.tfrecord*...:   …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/262144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/patch_camelyon/2.0.0.incompleteDEKBZI/patch_camelyon-train.tfrecord*...:  …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation examples...:   0%|          | 0/32768 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/patch_camelyon/2.0.0.incompleteDEKBZI/patch_camelyon-validation.tfrecord*.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset patch_camelyon downloaded and prepared to /root/tensorflow_datasets/patch_camelyon/2.0.0. Subsequent calls will reuse this data.\u001b[0m\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707052233.983190      97 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"313/313 [==============================] - 208s 529ms/step - loss: 0.6740 - accuracy: 0.7281 - val_loss: 0.9611 - val_accuracy: 0.6185\nEpoch 2/10\n313/313 [==============================] - 155s 496ms/step - loss: 0.4968 - accuracy: 0.7812 - val_loss: 0.9349 - val_accuracy: 0.6574\nEpoch 3/10\n313/313 [==============================] - 156s 500ms/step - loss: 0.4410 - accuracy: 0.8083 - val_loss: 0.7703 - val_accuracy: 0.7291\nEpoch 4/10\n313/313 [==============================] - 155s 498ms/step - loss: 0.4331 - accuracy: 0.8174 - val_loss: 0.9539 - val_accuracy: 0.6053\nEpoch 5/10\n313/313 [==============================] - 155s 497ms/step - loss: 0.3858 - accuracy: 0.8347 - val_loss: 0.8522 - val_accuracy: 0.6926\nEpoch 6/10\n313/313 [==============================] - 155s 497ms/step - loss: 0.4129 - accuracy: 0.8318 - val_loss: 0.6585 - val_accuracy: 0.7141\nEpoch 7/10\n313/313 [==============================] - 157s 502ms/step - loss: 0.3391 - accuracy: 0.8618 - val_loss: 0.8411 - val_accuracy: 0.6341\nEpoch 8/10\n313/313 [==============================] - 155s 496ms/step - loss: 0.4203 - accuracy: 0.8299 - val_loss: 1.4101 - val_accuracy: 0.5008\nEpoch 9/10\n313/313 [==============================] - 156s 498ms/step - loss: 0.3451 - accuracy: 0.8542 - val_loss: 0.4754 - val_accuracy: 0.7791\nEpoch 10/10\n313/313 [==============================] - 154s 493ms/step - loss: 0.2903 - accuracy: 0.8813 - val_loss: 0.5086 - val_accuracy: 0.8072\n7880/7880 [==============================] - 148s 19ms/step - loss: 0.5086 - accuracy: 0.8072\nValidation Loss: 0.5085952877998352\nValidation Accuracy: 0.8072490096092224\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Define the strategy for distributed training\nstrategy = tf.distribute.MirroredStrategy()\n\n# Load the Patch Camelyon dataset\ndataset, info = tfds.load('patch_camelyon', split='train', as_supervised=True, with_info=True)\n\n# Split the dataset into training and validation sets\ntrain_dataset = dataset.take(10000)\nval_dataset = dataset.skip(10000)\n\n# Define batch size per replica\nglobal_batch_size = 32\nbatch_size = global_batch_size // strategy.num_replicas_in_sync\n\n# Prepare datasets for distributed training\ntrain_dataset = train_dataset.cache().shuffle(buffer_size=10000).batch(batch_size)\nval_dataset = val_dataset.cache().batch(batch_size)\n\n# Define the model\ndef create_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(96, 96, 3)),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(2)  # No activation as we will use SparseCategoricalCrossentropy\n    ])\n    return model\n\n# Create and compile the model inside the strategy scope\nwith strategy.scope():\n    model = create_model()\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n# Define number of epochs\nnum_epochs = 10\n\n# Train the model\nmodel.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(val_dataset)\nprint(\"Validation Loss:\", loss)\nprint(\"Validation Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:43:23.175466Z","iopub.execute_input":"2024-02-04T13:43:23.176223Z","iopub.status.idle":"2024-02-04T13:48:58.393993Z","shell.execute_reply.started":"2024-02-04T13:43:23.176189Z","shell.execute_reply":"2024-02-04T13:48:58.393161Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch 1/10\n313/313 [==============================] - 99s 302ms/step - loss: 135.7145 - accuracy: 0.5121 - val_loss: 0.6947 - val_accuracy: 0.4999\nEpoch 2/10\n313/313 [==============================] - 23s 74ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6947 - val_accuracy: 0.4999\nEpoch 3/10\n313/313 [==============================] - 23s 75ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6946 - val_accuracy: 0.4999\nEpoch 4/10\n313/313 [==============================] - 23s 75ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6946 - val_accuracy: 0.4999\nEpoch 5/10\n313/313 [==============================] - 23s 74ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6945 - val_accuracy: 0.4999\nEpoch 6/10\n313/313 [==============================] - 23s 74ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6947 - val_accuracy: 0.4999\nEpoch 7/10\n313/313 [==============================] - 24s 75ms/step - loss: 0.6929 - accuracy: 0.5124 - val_loss: 0.6947 - val_accuracy: 0.4999\nEpoch 8/10\n313/313 [==============================] - 23s 75ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6948 - val_accuracy: 0.4999\nEpoch 9/10\n313/313 [==============================] - 23s 74ms/step - loss: 0.6928 - accuracy: 0.5124 - val_loss: 0.6946 - val_accuracy: 0.4999\nEpoch 10/10\n313/313 [==============================] - 23s 74ms/step - loss: 0.6929 - accuracy: 0.5124 - val_loss: 0.6946 - val_accuracy: 0.4999\n7880/7880 [==============================] - 26s 3ms/step - loss: 0.6946 - accuracy: 0.4999\nValidation Loss: 0.6946385502815247\nValidation Accuracy: 0.49988895654678345\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Define the strategy for distributed training\nstrategy = tf.distribute.MirroredStrategy()\n\n# Load the Cats vs Dogs dataset\ndataset_name = 'cats_vs_dogs'\n(train_dataset, test_dataset), info = tfds.load(name=dataset_name, \n                                                split=['train[:80%]', 'train[80%:]'], \n                                                with_info=True, \n                                                as_supervised=True)\n\n# Define batch size per replica\nglobal_batch_size = 32\nbatch_size = global_batch_size // strategy.num_replicas_in_sync\n\n# Define number of epochs\nnum_epochs = 10\n\n# Define a function to preprocess and resize images\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (150, 150))  # Resize images to a uniform size\n    image = tf.cast(image, tf.float32) / 255.0  # Normalize pixel values\n    return image, label\n\n# Apply preprocessing to the datasets\ntrain_dataset = train_dataset.map(preprocess_image).cache().shuffle(1000).batch(batch_size)\ntest_dataset = test_dataset.map(preprocess_image).cache().batch(batch_size)\n\n# Define the model\ndef create_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(150, 150, 3)),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')  \n    ])\n    return model\n\n# Create and compile the model inside the strategy scope\nwith strategy.scope():\n    model = create_model()\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_dataset, epochs=num_epochs)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(test_dataset)\nprint(\"Test Loss:\", loss)\nprint(\"Test Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:00:58.069916Z","iopub.execute_input":"2024-02-04T14:00:58.070364Z","iopub.status.idle":"2024-02-04T14:01:52.051144Z","shell.execute_reply.started":"2024-02-04T14:00:58.070329Z","shell.execute_reply":"2024-02-04T14:01:52.050127Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707055261.106148    2825 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"582/582 [==============================] - 15s 21ms/step - loss: 1.0564 - accuracy: 0.5452\nEpoch 2/10\n582/582 [==============================] - 4s 7ms/step - loss: 0.7890 - accuracy: 0.5636\nEpoch 3/10\n582/582 [==============================] - 4s 6ms/step - loss: 0.7271 - accuracy: 0.5892\nEpoch 4/10\n582/582 [==============================] - 4s 6ms/step - loss: 0.6895 - accuracy: 0.6000\nEpoch 5/10\n582/582 [==============================] - 4s 6ms/step - loss: 0.6537 - accuracy: 0.6221\nEpoch 6/10\n582/582 [==============================] - 4s 6ms/step - loss: 0.6412 - accuracy: 0.6368\nEpoch 7/10\n582/582 [==============================] - 4s 6ms/step - loss: 0.6409 - accuracy: 0.6300\nEpoch 8/10\n582/582 [==============================] - 4s 6ms/step - loss: 0.6359 - accuracy: 0.6377\nEpoch 9/10\n582/582 [==============================] - 4s 7ms/step - loss: 0.6359 - accuracy: 0.6368\nEpoch 10/10\n582/582 [==============================] - 4s 7ms/step - loss: 0.6370 - accuracy: 0.6327\n146/146 [==============================] - 3s 21ms/step - loss: 0.6396 - accuracy: 0.6298\nTest Loss: 0.6396113038063049\nTest Accuracy: 0.6298366189002991\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom sklearn.model_selection import train_test_split\n\n# Load Cats vs Dogs dataset\ndataset_name = 'cats_vs_dogs'\n(train_dataset, test_dataset), info = tfds.load(name=dataset_name, \n                                                split=['train[:80%]', 'train[80%:]'], \n                                                with_info=True, \n                                                as_supervised=True)\n\n# Preprocess images\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (150, 150))  # Resize images\n    image = tf.cast(image, tf.float32) / 255.0  # Normalize pixel values\n    return image, label\n\n# Apply preprocessing to the datasets\ntrain_dataset = train_dataset.map(preprocess_image)\ntest_dataset = test_dataset.map(preprocess_image)\n\n# Split train dataset into train and validation\ntrain_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2)\n\n# Define the model\ndef create_model(conv_filters=(32, 64), dense_units=128, learning_rate=0.001):\n    model = Sequential([\n        Conv2D(conv_filters[0], (3, 3), activation='relu', input_shape=(150, 150, 3)),\n        MaxPooling2D(2, 2),\n        Conv2D(conv_filters[1], (3, 3), activation='relu'),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(dense_units, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer=Adam(learning_rate),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# Wrap the model in a function for compatibility with GridSearchCV\ndef keras_model_builder(conv_filters=(32, 64), dense_units=128, learning_rate=0.001):\n    model = create_model(conv_filters=conv_filters, dense_units=dense_units, learning_rate=learning_rate)\n    return model\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'conv_filters': [(32, 64), (64, 128)],\n    'dense_units': [64, 128, 256],\n    'learning_rate': [0.001, 0.0001]\n}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator=keras_model_builder(), param_grid=param_grid, cv=3)\ngrid_search.fit(train_dataset, validation_data=val_dataset, epochs=5)\n\n# Summarize results\nprint(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n\n# Evaluate the best model\ntest_loss, test_acc = grid_search.best_estimator_.evaluate(test_dataset)\nprint(\"Test Accuracy:\", test_acc)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T12:34:00.204750Z","iopub.status.idle":"2024-02-04T12:34:00.205135Z","shell.execute_reply.started":"2024-02-04T12:34:00.204951Z","shell.execute_reply":"2024-02-04T12:34:00.204968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-02-04T12:34:00.210404Z","iopub.status.idle":"2024-02-04T12:34:00.211013Z","shell.execute_reply.started":"2024-02-04T12:34:00.210820Z","shell.execute_reply":"2024-02-04T12:34:00.210841Z"},"trusted":true},"execution_count":null,"outputs":[]}]}